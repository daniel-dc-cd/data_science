{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Natural Language Processing Pipeline\n",
    "\n",
    "Today we will build on what Hutaf did last week and combine NLP with what we have been doing with Random Forest Classifiers yesterday. We'll be using a text classificaton model on a dataset which contains real info on what corporations actually talk about on social media. This is the first time in this course we will build a pipeline to process data.\n",
    "\n",
    "##### The statements were labelled as into following categories\n",
    "\n",
    "- `information` (objective statements about the company or it's activities), \n",
    "- `dialog` (replies to users, etc.), or \n",
    "- `action` (messages that ask for votes or ask users to click on links, etc.). \n",
    "\n",
    "Our aim is to build a model to automatically categorize the text into their respective categories. \n",
    "\n",
    "##### Get the data\n",
    "\n",
    "You can download the dataset from [here](https://data.world/crowdflower/corporate-messaging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Load the data and examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see head of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe shape of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of target column i.e. category\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of the column - category_confidence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove those observations where category_confidence < 1 and category = Exclude\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features i.e the column - text and target i.e the column - category\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Text preprocessing\n",
    "\n",
    "We will do the below pre-processing tasks on the text\n",
    "- tokenizing the sentences\n",
    "- replace the urls with a placeholder\n",
    "- removing non ascii characters\n",
    "- text normalizing using lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's observe a text in the dataset, extract the first text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now extract the third text from this dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re library for regular expressions\n",
    "\n",
    "\n",
    "# import nltk library\n",
    "\n",
    "\n",
    "# import stopwords from nltk library\n",
    "\n",
    "\n",
    "# download the stopwords and wordnet corpus\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# extract the english stopwords and save it to a variable\n",
    "\n",
    "\n",
    "# import word_tokenize from nltk library\n",
    "\n",
    "\n",
    "# import WordNetLemmatizer from nltk library\n",
    "\n",
    "\n",
    "# write a regular expression to identify urls in text\n",
    "url_regex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "# write a regular expression to identify non-ascii characters in text\n",
    "non_ascii_regex = r'[^\\x00-\\x7F]+'\n",
    "\n",
    "# write a function to tokenize text after performing preprocessing \n",
    "def tokenize(text):\n",
    "    \n",
    "    # use library re to replace urls by token - urlplaceholder\n",
    "    \n",
    "    \n",
    "    # use library re to replace non ascii characters by a space\n",
    "      \n",
    "\n",
    "    # use word_tokenize to tokenize the sentences\n",
    "    \n",
    "    \n",
    "    # instantiate an object of class WordNetLemmatizer\n",
    "    \n",
    "\n",
    "    # use a list comprehension to lemmatize the tokens and remove the the stopwords\n",
    "    \n",
    "\n",
    "    # return the tokens\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will do exploratory data analysis to check if there are any new features that we can generate based on the existing text that we have in the dataset. I have come up with these two assumptions. What others can you think of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis 1:** The length of the text in each category might be different from each other\n",
    "<br>**Hypothesis 2:** The total number of URLs that are present in text might be different in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column in the original dataset - 'length' to capture length of each text\n",
    "\n",
    "\n",
    "# use seaborn boxplot to visualize the pattern in length for each category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column in the original dataset - 'url_count' to capture total count of urls present in each text\n",
    "\n",
    "\n",
    "# use pandas crosstab to see the distibution of different url counts in each category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Creating custom transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An estimator is any object that learns from data, whether it's a classification, regression, or clustering algorithm, or a transformer that extracts or filters useful features from raw data. Since estimators learn from data, they each must have a `fit` method that takes a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two kinds of estimators - `Transformer Estimators` i.e. transformers in short and `Predictor Estimators` i.e. predictor in short. In transformers we also need to have another method `transform` and predictors need to have another method `predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of `transformers` are - CountVectorizer, TfidfVectorizer, MinMaxScaler, StandardScaler etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of `predictors` are - LinearRegression, LogisticRegression, RandomForestClassifier etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom transformer LengthExtractor to extract length of each sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom transformer UrlCounter to count number of urls in each sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Model Building using FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature union applies a list of transformer objects in parallel to the input data, then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](pipeline.png \"nlp pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import RandomForestClassifier from sklearn\n",
    "\n",
    "\n",
    "# import Pipeline and FeatureUnion from sklearn\n",
    "\n",
    "\n",
    "# import CountVectorizer, TfidfTransformer from sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of Pipeline class\n",
    "\n",
    "    \n",
    "        # create a FeatureUnion pipeline\n",
    "        \n",
    "\n",
    "            # add a pipeline element to extract features using CountVectorizer and TfidfTransformer\n",
    "            \n",
    "\n",
    "            # add the pipeline element - LengthExtractor to extract lenght of each sentence as feature\n",
    "            \n",
    "            \n",
    "            # add another pipeline element - UrlCounter to extract url counts in each sentence as feature\n",
    "            \n",
    "\n",
    "        # use the predictor estimator RandomForestClassifier to train the model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pipeline.fit method to train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once the model is trained, in this task we will evaluate how the model behaves in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the method pipeline.predict on X_test data to predict the labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the confustion matrix, import confusion_matrix from sklearn\n",
    "\n",
    "\n",
    "# count the number of labels\n",
    "\n",
    "\n",
    "# use sns.heatmap on top of confusion_matrix to show the confusuin matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the classification report, import classification_report from sklearn\n",
    "\n",
    "\n",
    "# apply the function classification_report on y_test, y_pred and print it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Conclusion and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you take this further? What more can you do?\n",
    "\n",
    "- hyper parameter tuning\n",
    "- more feature engineering\n",
    "- feature selection\n",
    "- trying different predictors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
